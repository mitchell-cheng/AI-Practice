{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9385ac1-45cd-4ed4-b9fb-ce542d2f95d7",
   "metadata": {},
   "source": [
    "<h1>Gradient Checking</h1>\n",
    "<ul>\n",
    "    <li>1-Dimensional Gradient Checking</li>\n",
    "    <li>N-Dimensional Gradient Checking</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720aa78-583c-4e7c-b455-db3b90609223",
   "metadata": {},
   "source": [
    "<h2>Import Dependencies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72eabf7-0d31-4203-b7f0-37ae8940e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gradient_checking_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1739f3-d53f-43b1-a9ef-99967fb5a02c",
   "metadata": {},
   "source": [
    "<h2>1-Dimensional Gradient Checking</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d163a3-1d63-445b-a35e-0dbdb7516de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    '''\n",
    "    simulate a simple function runs forward propagation\n",
    "    \n",
    "    Args:\n",
    "    x -- input\n",
    "    theta -- parameter\n",
    "    \n",
    "    Returns:\n",
    "    J -- output\n",
    "    '''\n",
    "    \n",
    "    J = x * theta\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e205a80-bf9b-444d-a957-cbccdf7948e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8\n"
     ]
    }
   ],
   "source": [
    "# test the forward propagation\n",
    "\n",
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2518d322-6c23-438f-ae48-b648a971afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, theta):\n",
    "    '''\n",
    "    simulate a simple function runs backward propagation\n",
    "    \n",
    "    Args:\n",
    "    x -- input\n",
    "    theta -- parameter\n",
    "    \n",
    "    Returns:\n",
    "    dtheta -- gradient\n",
    "    '''\n",
    "    \n",
    "    dtheta = x\n",
    "    \n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed31049e-fbb8-4f9f-af45-588f3596c59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 3\n"
     ]
    }
   ],
   "source": [
    "# test the backward propagation\n",
    "\n",
    "x, theta = 3, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ec1164b-8485-423d-b40a-9999d5b36c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon, threshold):\n",
    "    '''\n",
    "    using gradient check to testify the gradient is correctly calculated\n",
    "    \n",
    "    Args:\n",
    "    theta -- parameter\n",
    "    J -- forward pass output\n",
    "    dtheta -- backward prop gradient \n",
    "    epsilon -- a very small number\n",
    "    threshold -- a number to determine whether the gradients difference is small enough\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference between two gradients\n",
    "    '''\n",
    "    \n",
    "    theta_plus = theta + epsilon\n",
    "    theta_minus = theta - epsilon\n",
    "    J_plus = forward_propagation(x, theta_plus)\n",
    "    J_minus = forward_propagation(x, theta_minus)\n",
    "    dtheta = backward_propagation(x, theta)\n",
    "    gradapprox = (J_plus - J_minus) / (2 * epsilon)\n",
    "    difference = np.linalg.norm((dtheta - gradapprox), ord=None) / (np.linalg.norm(dtheta, ord=None) + np.linalg.norm(gradapprox, ord=None))\n",
    "    \n",
    "    if difference <= threshold:\n",
    "        print('The backward propagation is working fine')\n",
    "    else:\n",
    "        print('There is something wrong with the gradient')\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4edfd8c-a435-4244-b6b3-329dc8db5439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The backward propagation is working fine\n",
      "the difference: 7.814075313343006e-11\n"
     ]
    }
   ],
   "source": [
    "x, theta = 3, 4\n",
    "difference = gradient_check(x, theta, 1e-7, 2e-7)\n",
    "print(f'the difference: {difference}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbe925-2c23-4ebd-bfff-716e91e5bdae",
   "metadata": {},
   "source": [
    "<h2>N-Dimensional Gradient Checking</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2487a331-a8e8-48d6-b449-39092f91c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- training set for m examples\n",
    "    Y -- labels for m examples \n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the cost function (logistic cost for m examples)\n",
    "    cache -- a tuple with the intermediate values (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    # Cost\n",
    "    log_probs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1. / m * np.sum(log_probs)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec28e169-4c86-4c2e-b66b-f725c75408fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input datapoint, of shape\n",
    "    Y -- true \"label\"\n",
    "    cache -- cache output from forward_propagation_n()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f456711-a7a0-4f24-a768-a6993b2dc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(X, Y, parameters, gradients, epsilon, threshold):\n",
    "    '''\n",
    "    check if the backward propagation is correctly implemented\n",
    "    \n",
    "    Args:\n",
    "    X -- training data\n",
    "    Y -- training label\n",
    "    parameters -- weights and bais\n",
    "    gradients -- gradients \n",
    "    epsilon -- the increment\n",
    "    threshold -- difference to determine if the two gradients are close enough\n",
    "    \n",
    "    Returns:\n",
    "    difference -- the difference between two gradients\n",
    "    '''\n",
    "    \n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    for i in range(num_parameters):\n",
    "        theta_plus = np.copy(parameters_values)\n",
    "        theta_plus[i] = theta_plus[i] + epsilon\n",
    "        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))\n",
    "        \n",
    "        theta_minus = np.copy(parameters_values)\n",
    "        theta_minus[i] = theta_minus[i] - epsilon\n",
    "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))\n",
    "        \n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "\n",
    "    difference = np.linalg.norm(grad - gradapprox, ord=None) / (np.linalg.norm(grad, ord=None) + np.linalg.norm(gradapprox, ord=None))\n",
    "    \n",
    "    if difference >= threshold:\n",
    "        print('There is something wrong with the backward propagation')\n",
    "    else:\n",
    "        print('Backward propagation works fine')\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b650352a-6cff-4960-9354-e273a4ccac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(4,3)\n",
    "Y = np.array([1, 1, 0])\n",
    "W1 = np.random.randn(5,4) \n",
    "b1 = np.random.randn(5,1) \n",
    "W2 = np.random.randn(3,5) \n",
    "b2 = np.random.randn(3,1) \n",
    "W3 = np.random.randn(1,3) \n",
    "b3 = np.random.randn(1,1) \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b627af89-ae04-46b9-8ca0-40f7c12b0699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward propagation works fine\n",
      "the difference: 1.1885552035482147e-07\n"
     ]
    }
   ],
   "source": [
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(X, Y, parameters, gradients, 1e-7, 2e-7)\n",
    "expected_values = [0.2850931567761623, 1.1890913024229996e-07]\n",
    "print(f'the difference: {difference}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
