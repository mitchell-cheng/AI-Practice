{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f11882-9151-473e-9ff2-dac6587e601f",
   "metadata": {},
   "source": [
    "<b>Import dependencies</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b689bc5-f510-468e-a893-30766e619d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from dnn_utils import sigmoid, relu, sigmoid_backward, relu_backward\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b06a78-4ca5-486b-a429-0c824ee13462",
   "metadata": {},
   "source": [
    "<b>Initialization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c38d60-f9be-43c1-8c72-aaf1228c6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init two-layer model parameters\n",
    "\n",
    "def init_parameters(n_x, n_h, n_y):\n",
    "    '''\n",
    "    random init for weights, init with zeros for bias\n",
    "    \n",
    "    Args:\n",
    "    n_x -- size of input layer\n",
    "    n_h -- neurons of hidden layer\n",
    "    n_y -- neurons of output layer\n",
    "    \n",
    "    Return:\n",
    "    parameters -- a python dictionary contains model parameters\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9732e874-2b0b-4cce-b503-1e4182733367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      " [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]] and shape (2, 3)\n",
      "b1:\n",
      " [[0.]\n",
      " [0.]] and shape (2, 1)\n",
      "W2:\n",
      " [[ 0.01744812 -0.00761207]] and shape (1, 2)\n",
      "b2:\n",
      " [[0.]] and shape (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# test init function\n",
    "\n",
    "parameters = init_parameters(3, 2, 1)\n",
    "print(f'W1:\\n {parameters[\"W1\"]} and shape {parameters[\"W1\"].shape}')\n",
    "print(f'b1:\\n {parameters[\"b1\"]} and shape {parameters[\"b1\"].shape}')\n",
    "print(f'W2:\\n {parameters[\"W2\"]} and shape {parameters[\"W2\"].shape}')\n",
    "print(f'b2:\\n {parameters[\"b2\"]} and shape {parameters[\"b2\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eaa3638-85e2-4b96-9cac-597592907b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init l-layers model parameters\n",
    "\n",
    "def init_l_layer_parameters(layer_dims):\n",
    "    '''\n",
    "    random init for weights, init with zeros for bias\n",
    "    \n",
    "    Args:\n",
    "    layer_dims -- a list contains layer dimensions\n",
    "    \n",
    "    Return:\n",
    "    parameters -- a python dictionary contains model parameters\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters['W'+str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01\n",
    "        parameters['b'+str(i)] = np.zeros((layer_dims[i], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f69621f-79fa-40d8-a3ab-cbeeaa95b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 has [[ 0.01624345 -0.00611756 -0.00528172 -0.01072969  0.00865408]\n",
      " [-0.02301539  0.01744812 -0.00761207  0.00319039 -0.0024937 ]\n",
      " [ 0.01462108 -0.02060141 -0.00322417 -0.00384054  0.01133769]\n",
      " [-0.01099891 -0.00172428 -0.00877858  0.00042214  0.00582815]] and shape (4, 5)\n",
      "b1 has [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] and shape (4, 1)\n",
      "W2 has [[-0.01100619  0.01144724  0.00901591  0.00502494]\n",
      " [ 0.00900856 -0.00683728 -0.0012289  -0.00935769]\n",
      " [-0.00267888  0.00530355 -0.00691661 -0.00396754]] and shape (3, 4)\n",
      "b2 has [[0.]\n",
      " [0.]\n",
      " [0.]] and shape (3, 1)\n",
      "W3 has [[-0.00687173 -0.00845206 -0.00671246]\n",
      " [-0.00012665 -0.0111731   0.00234416]] and shape (2, 3)\n",
      "b3 has [[0.]\n",
      " [0.]] and shape (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# test init function\n",
    "\n",
    "parameters = init_l_layer_parameters([5,4,3,2])\n",
    "for i in parameters:\n",
    "    print(f'{i} has {parameters[i]} and shape {parameters[i].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2a106-1cf8-40a9-a3e2-ddd9756b22a5",
   "metadata": {},
   "source": [
    "<b>Forward propagation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8a575de-354c-491b-ba41-a1177e4b9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    '''\n",
    "    perform linear forward propagation\n",
    "    \n",
    "    Args:\n",
    "    A -- previous layer input\n",
    "    W -- current layer weights\n",
    "    b -- current layer bias\n",
    "    \n",
    "    Return:\n",
    "    Z -- linear forward output \n",
    "    linear_cache -- intermediate variable for backward propagation\n",
    "    '''\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    linear_cache = (A, W, b)\n",
    "    \n",
    "    return Z, linear_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d6917ea-0c31-4d8c-8bed-0f53373d9cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear forward output:\n",
      " [[0.00063885 0.00926233]] and shape (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# test linear_forward\n",
    "\n",
    "A_test = np.random.randn(3,2)\n",
    "W_test = np.random.randn(1,3) * 0.01\n",
    "b_test = np.zeros((1,1))\n",
    "Z_test, linear_cache = linear_forward(A_test, W_test, b_test)\n",
    "\n",
    "print(f'linear forward output:\\n {Z_test} and shape {Z_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c2e60fe-da9d-4c50-8231-a60d5b614885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    '''\n",
    "    perform linear forward propagation with non-linear\n",
    "    \n",
    "    Args:\n",
    "    A_prev -- previous layer input\n",
    "    W -- current layer weights\n",
    "    b -- current layer bais\n",
    "    activation -- type of activation function\n",
    "    \n",
    "    Return:\n",
    "    A -- linear_activation output\n",
    "    linear_activation_cache -- intermediate variable for backward propagation\n",
    "    '''\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    linear_activation_cache = (linear_cache, activation_cache)\n",
    "    return A, linear_activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "574ff667-dd50-4b50-88a0-7ca0f8a1ee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid output:\n",
      " [[0.50015971 0.50231557]] and shape (1, 2)\n",
      "relu output:\n",
      " [[0.00063885 0.00926233]] and shape (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# test linear_activation forward function\n",
    "\n",
    "sigmoid_output, sigmoid_linear_activation_cache = linear_activation_forward(A_test, W_test, b_test, 'sigmoid')\n",
    "relu_output, relu_linear_activation_cache = linear_activation_forward(A_test, W_test, b_test, 'relu')\n",
    "print(f'sigmoid output:\\n {sigmoid_output} and shape {sigmoid_output.shape}')\n",
    "print(f'relu output:\\n {relu_output} and shape {relu_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44355191-d759-4566-bc90-751ef0ef9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-layer linear_activation_forward\n",
    "\n",
    "def l_layer_linear_activation_forward(X, parameters):\n",
    "    '''\n",
    "    perform l-pass forward propagation with non-linear\n",
    "    \n",
    "    Args:\n",
    "    X -- original input\n",
    "    parameters -- a dictionary contains parameters\n",
    "    \n",
    "    Return:\n",
    "    A -- linear_activation output\n",
    "    cache -- intermediate variable for backward propagation\n",
    "    '''\n",
    "    A = X \n",
    "    L = len(parameters) // 2\n",
    "    cache = []\n",
    "    for i in range(1, L):\n",
    "        A_prev = A\n",
    "        A, linear_activation_cache = linear_activation_forward(A_prev, parameters['W'+str(i)], parameters['b'+str(i)], 'relu')\n",
    "        cache.append(linear_activation_cache)\n",
    "        \n",
    "    A_out, linear_activation_cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], 'sigmoid')\n",
    "    cache.append(linear_activation_cache)\n",
    "        \n",
    "    return A_out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c618e9d8-3300-47e5-b519-b89a5c1283df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the network output:\n",
      " [[0.4999999  0.49999877 0.49999914]\n",
      " [0.49999989 0.50000011 0.50000007]] and has shape (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# test l-layer linear_activation_forward\n",
    "\n",
    "output, cache = l_layer_linear_activation_forward(np.random.randn(5,3), parameters)\n",
    "print(f'the network output:\\n {output} and has shape {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454582c2-aa0b-4e44-94a9-9393720f7822",
   "metadata": {},
   "source": [
    "<b>Cost function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "631b45c7-99d1-4915-91bf-7d135a346330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y, Y_predict):\n",
    "    '''\n",
    "    compute model cost\n",
    "    \n",
    "    Args:\n",
    "    Y -- ground-truth labels\n",
    "    Y_predict -- model output\n",
    "    \n",
    "    Return:\n",
    "    cost -- model cost\n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(Y * np.log(Y_predict) + (1-Y) * np.log(1 - Y_predict)) / m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85ab3f88-ed7a-4e6c-bf6c-6bee307efea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model cost: 1.3862929573092737\n"
     ]
    }
   ],
   "source": [
    "# test cost\n",
    "\n",
    "cost = compute_cost(np.zeros((1,3)), output)\n",
    "print(f'model cost: {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7f9ad-63b5-4880-8631-2bf2f8a114fc",
   "metadata": {},
   "source": [
    "<b>Back propagation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8c0adce-8df3-4efa-a7b9-39646198ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_backward\n",
    "\n",
    "def linear_backward(dZ, linear_cache):\n",
    "    '''\n",
    "    perform linear backward propagation\n",
    "    \n",
    "    Args:\n",
    "    dZ -- gradient for Z\n",
    "    linear_cache -- intermediate variables for backward propagation\n",
    "    \n",
    "    Return:\n",
    "    linear_grads -- gradients\n",
    "    '''\n",
    "    \n",
    "    A, W, b = linear_cache\n",
    "    m = A.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ, A.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ) \n",
    "    \n",
    "    linear_grads = {\n",
    "        'dW': dW,\n",
    "        'db': db,\n",
    "        'dA_prev': dA_prev\n",
    "    }\n",
    "    \n",
    "    return linear_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e993255d-4756-45df-b4a7-a3caca3ebdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gradients for weights:\n",
      " [[ 1.19695646 -0.53556022  0.46504878]] and has shape (1, 3)\n",
      "the gradients for bias:\n",
      " [[0.99504941]] and has shape (1, 1)\n",
      "the gradients for previous layer output:\n",
      " [[ 0.00050775  0.00050337]\n",
      " [-0.00636589 -0.00631096]\n",
      " [ 0.00190794  0.00189147]] and has shape (3, 2)\n"
     ]
    }
   ],
   "source": [
    "# test linear_backward\n",
    "\n",
    "linear_grads = linear_backward(1-Z_test, linear_cache)\n",
    "print(f'the gradients for weights:\\n {linear_grads[\"dW\"]} and has shape {linear_grads[\"dW\"].shape}')\n",
    "print(f'the gradients for bias:\\n {linear_grads[\"db\"]} and has shape {linear_grads[\"db\"].shape}')\n",
    "print(f'the gradients for previous layer output:\\n {linear_grads[\"dA_prev\"]} and has shape {linear_grads[\"dA_prev\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5985b130-3bb0-40c0-930c-e58d11dd90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, linear_activation_cache, activation):\n",
    "    '''\n",
    "    perform linear backward propagation with non-linear\n",
    "    \n",
    "    Args:\n",
    "    dA -- gradient for A\n",
    "    linear_activation_cache -- intermediate variables for backward propagation\n",
    "    activation -- decide to pick relu or sigmoid\n",
    "    \n",
    "    Return:\n",
    "    linear_activation_grads -- gradients\n",
    "    '''\n",
    "    (linear_cache, activation_cache) = linear_activation_cache\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        linear_activation_grads = linear_backward(dZ, linear_cache)\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        linear_activation_grads = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return linear_activation_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3507190d-a355-4d47-bb76-71a9917bf482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid gradients:\n",
      " {'dW': array([[ 0.1503623 , -0.06772609,  0.0595537 ]]), 'db': array([[0.12530806]]), 'dA_prev': array([[ 6.35299736e-05,  6.38024469e-05],\n",
      "       [-7.96498818e-04, -7.99914916e-04],\n",
      "       [ 2.38720561e-04,  2.39744408e-04]])}\n",
      "relu gradients:\n",
      " {'dW': array([[ 0.00396671, -0.00417203,  0.00759938]]), 'db': array([[0.00495059]]), 'dA_prev': array([[ 3.24586083e-07,  4.70598280e-06],\n",
      "       [-4.06945598e-06, -5.90006500e-05],\n",
      "       [ 1.21966636e-06,  1.76832255e-05]])}\n"
     ]
    }
   ],
   "source": [
    "# test linear_activation_backward\n",
    "\n",
    "sigmoid_grads = linear_activation_backward(sigmoid_output, sigmoid_linear_activation_cache, 'sigmoid')\n",
    "relu_grads = linear_activation_backward(relu_output, relu_linear_activation_cache, 'relu')\n",
    "print(f'sigmoid gradients:\\n {sigmoid_grads}')\n",
    "print(f'relu gradients:\\n {relu_grads}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "914bffea-2733-45e6-8438-cc1c70259c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-layer linear_activation_backward\n",
    "\n",
    "def l_layer_linear_activation_backward(Y_predict, Y, cache):\n",
    "    '''\n",
    "    perform l-pass forward propagation with non-linear\n",
    "    \n",
    "    Args:\n",
    "    Y_predict -- forward pass output \n",
    "    Y -- ground-truth label\n",
    "    cache -- intermediate cache\n",
    "    \n",
    "    Return:\n",
    "    grads -- A dictionary with the gradients\n",
    "    '''\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(cache)\n",
    "    Y = Y.reshape(Y_predict.shape)\n",
    "    \n",
    "    dloss = - (np.divide(Y, Y_predict) - np.divide(1-Y, 1-Y_predict))\n",
    "    current_cache = cache[L-1] \n",
    "    linear_activation_grads = linear_activation_backward(dloss, current_cache, 'sigmoid')\n",
    "    dW = linear_activation_grads['dW']\n",
    "    db = linear_activation_grads['db']\n",
    "    dA_prev = linear_activation_grads['dA_prev']\n",
    "    grads['dW' + str(L)] = dW \n",
    "    grads['db' + str(L)] = db \n",
    "    grads['dA' + str(L-1)] = dA_prev \n",
    "    \n",
    "    for i in reversed(range(L-1)):\n",
    "        current_cache = cache[i]\n",
    "        linear_activation_grads = linear_activation_backward(grads['dA' + str(i+1)], current_cache, 'relu')\n",
    "        dW = linear_activation_grads['dW']\n",
    "        db = linear_activation_grads['db']\n",
    "        dA_prev = linear_activation_grads['dA_prev']\n",
    "        grads['dW' + str(i+1)] = dW\n",
    "        grads['db' + str(i+1)] = db\n",
    "        grads['dA' + str(i)] = dA_prev\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89b970b9-6242-4813-8da6-5bf42cc3e4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads {'dW3': array([[ 2.21461081e-05, -6.55843207e-06,  1.23840881e-05],\n",
      "       [ 1.41430411e-04, -6.55843223e-06,  6.07893675e-05]]), 'db3': array([[-0.16666739],\n",
      "       [ 0.16666669]]), 'dA2': array([[ 0.00349919, -0.00349918,  0.00337255],\n",
      "       [ 0.00981258, -0.00981257, -0.00136052],\n",
      "       [ 0.00218415, -0.00218414,  0.00452832]]), 'dW2': array([[ 6.29242964e-06, -1.63789215e-05,  8.77805608e-06,\n",
      "         1.49520943e-06],\n",
      "       [ 1.76455213e-05,  0.00000000e+00,  2.46158295e-05,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  1.38681524e-05,  0.00000000e+00,\n",
      "         3.57360303e-06]]), 'db2': array([[0.00112419],\n",
      "       [0.00327086],\n",
      "       [0.00078139]]), 'dA1': array([[ 4.98845036e-05,  4.43636856e-05, -4.92497087e-05],\n",
      "       [-2.70353316e-05, -5.16396465e-05,  6.26225048e-05],\n",
      "       [ 1.94896428e-05, -1.64414004e-05, -9.14012761e-07],\n",
      "       [-7.42399256e-05, -8.91750474e-06, -1.01939507e-06]]), 'dW1': array([[ 8.52908316e-06, -1.25661913e-06,  3.63421148e-05,\n",
      "        -8.38834306e-06,  5.24843089e-06],\n",
      "       [ 1.53285462e-05,  1.22459314e-05, -6.10648217e-06,\n",
      "         1.55345449e-05,  2.84168313e-05],\n",
      "       [ 3.33227299e-06, -4.90955230e-07,  1.41986946e-05,\n",
      "        -3.27728648e-06,  2.05053745e-06],\n",
      "       [ 7.20083760e-07, -3.88020140e-06,  4.64179506e-06,\n",
      "        -7.73431200e-07,  6.11504393e-06]]), 'db1': array([[ 1.66281679e-05],\n",
      "       [ 3.66095275e-06],\n",
      "       [ 6.49654759e-06],\n",
      "       [-3.31229993e-06]]), 'dA0': array([[ 1.09525624e-06,  1.28658930e-06, -1.43006894e-06],\n",
      "       [-7.06685715e-07, -8.85638334e-07,  1.09440256e-06],\n",
      "       [-3.26313818e-07,  4.71367619e-07, -4.67737983e-07],\n",
      "       [-6.10095893e-07, -1.68515074e-07,  1.99359948e-07],\n",
      "       [ 6.52671914e-07,  7.68014062e-08, -1.62103165e-07]])}\n"
     ]
    }
   ],
   "source": [
    "# test l-layer linear_activation_backward\n",
    "\n",
    "grads = l_layer_linear_activation_backward(output, np.array([[1, 0, 1],\n",
    "                                                            [1, 0, 0]]), cache)\n",
    "print(f'grads {grads}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0c66a-5e75-40ab-83e5-f815f6454cce",
   "metadata": {},
   "source": [
    "<b>Update parameters</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "555910e7-a2cc-49f6-ae15-6018a322f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate):\n",
    "    '''\n",
    "    update weights and bias\n",
    "    \n",
    "    Args:\n",
    "    parameters -- weights and bias\n",
    "    grads -- gradients for weights and bias\n",
    "    learning_rate -- learning rate alpha\n",
    "    \n",
    "    Return:\n",
    "    updated_parameters -- updated parameters\n",
    "    '''\n",
    "    \n",
    "    L = len(params) // 2\n",
    "    parameters = params.copy()\n",
    "    for i in range(L):\n",
    "        parameters['W' + str(i+1)] = parameters['W' + str(i+1)] - learning_rate * grads['dW' + str(i+1)]\n",
    "        parameters['b' + str(i+1)] = parameters['b' + str(i+1)] - learning_rate * grads['db' + str(i+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba49363e-9ffd-4846-b28c-9a447a0ebc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated parameters:\n",
      " {'W1': array([[ 0.0162426 , -0.00611744, -0.00528535, -0.01072885,  0.00865355],\n",
      "       [-0.02301692,  0.01744689, -0.00761146,  0.00318884, -0.00249655],\n",
      "       [ 0.01462075, -0.02060136, -0.00322559, -0.00384022,  0.01133749],\n",
      "       [-0.01099898, -0.00172389, -0.00877905,  0.00042221,  0.00582754]]), 'b1': array([[-1.66281679e-06],\n",
      "       [-3.66095275e-07],\n",
      "       [-6.49654759e-07],\n",
      "       [ 3.31229993e-07]]), 'W2': array([[-0.01100682,  0.01144887,  0.00901503,  0.00502479],\n",
      "       [ 0.00900679, -0.00683728, -0.00123136, -0.00935769],\n",
      "       [-0.00267888,  0.00530217, -0.00691661, -0.00396789]]), 'b2': array([[-1.12418516e-04],\n",
      "       [-3.27086067e-04],\n",
      "       [-7.81390469e-05]]), 'W3': array([[-0.00687394, -0.0084514 , -0.0067137 ],\n",
      "       [-0.00014079, -0.01117245,  0.00233808]]), 'b3': array([[ 0.01666674],\n",
      "       [-0.01666667]])}\n"
     ]
    }
   ],
   "source": [
    "# test update parameters\n",
    "\n",
    "updated_parameters = update_params(parameters, grads, 0.1)\n",
    "print(f'updated parameters:\\n {updated_parameters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80339012-7510-4b68-96b3-1bcbb77cba2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
