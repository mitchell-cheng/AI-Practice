{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f11882-9151-473e-9ff2-dac6587e601f",
   "metadata": {},
   "source": [
    "<b>Import dependencies</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7b689bc5-f510-468e-a893-30766e619d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from dnn_utils import sigmoid, relu, sigmoid_backward, relu_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b06a78-4ca5-486b-a429-0c824ee13462",
   "metadata": {},
   "source": [
    "<b>Initialization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28c38d60-f9be-43c1-8c72-aaf1228c6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init two-layer model parameters\n",
    "\n",
    "def init_parameters(n_x, n_h, n_y):\n",
    "    '''\n",
    "    random init for weights, init with zeros for bias\n",
    "    \n",
    "    Args:\n",
    "    n_x -- size of input layer\n",
    "    n_h -- neurons of hidden layer\n",
    "    n_y -- neurons of output layer\n",
    "    \n",
    "    Return:\n",
    "    parameters -- a python dictionary contains model parameters\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9732e874-2b0b-4cce-b503-1e4182733367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      " [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]] and shape (2, 3)\n",
      "b1:\n",
      " [[0.]\n",
      " [0.]] and shape (2, 1)\n",
      "W2:\n",
      " [[ 0.01744812 -0.00761207]] and shape (1, 2)\n",
      "b2:\n",
      " [[0.]] and shape (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# test init function\n",
    "\n",
    "parameters = init_parameters(3, 2, 1)\n",
    "print(f'W1:\\n {parameters[\"W1\"]} and shape {parameters[\"W1\"].shape}')\n",
    "print(f'b1:\\n {parameters[\"b1\"]} and shape {parameters[\"b1\"].shape}')\n",
    "print(f'W2:\\n {parameters[\"W2\"]} and shape {parameters[\"W2\"].shape}')\n",
    "print(f'b2:\\n {parameters[\"b2\"]} and shape {parameters[\"b2\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eaa3638-85e2-4b96-9cac-597592907b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init l-layers model parameters\n",
    "\n",
    "def init_l_layer_parameters(layer_dims):\n",
    "    '''\n",
    "    random init for weights, init with zeros for bias\n",
    "    \n",
    "    Args:\n",
    "    layer_dims -- a list contains layer dimensions\n",
    "    \n",
    "    Return:\n",
    "    parameters -- a python dictionary contains model parameters\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters['W'+str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01\n",
    "        parameters['b'+str(i)] = np.zeros((layer_dims[i], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f69621f-79fa-40d8-a3ab-cbeeaa95b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 has [[ 0.01788628  0.0043651 ]\n",
      " [ 0.00096497 -0.01863493]\n",
      " [-0.00277388 -0.00354759]\n",
      " [-0.00082741 -0.00627001]\n",
      " [-0.00043818 -0.00477218]] and shape (5, 2)\n",
      "b1 has [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] and shape (5, 1)\n",
      "W2 has [[-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]\n",
      " [-0.01185047 -0.0020565   0.01486148  0.00236716 -0.01023785]\n",
      " [-0.00712993  0.00625245 -0.00160513 -0.00768836 -0.00230031]] and shape (4, 5)\n",
      "b2 has [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] and shape (4, 1)\n",
      "W3 has [[ 0.00745056  0.01976111 -0.01244123 -0.00626417]\n",
      " [-0.00803766 -0.02419083 -0.00923792 -0.01023876]\n",
      " [ 0.01123978 -0.00131914 -0.01623285  0.00646675]] and shape (3, 4)\n",
      "b3 has [[0.]\n",
      " [0.]\n",
      " [0.]] and shape (3, 1)\n",
      "W4 has [[-0.00356271 -0.01743141 -0.0059665 ]\n",
      " [-0.00588594 -0.00873882  0.00029714]] and shape (2, 3)\n",
      "b4 has [[0.]\n",
      " [0.]] and shape (2, 1)\n",
      "W5 has [[-0.02248258 -0.00267762]] and shape (1, 2)\n",
      "b5 has [[0.]] and shape (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# test init function\n",
    "\n",
    "parameters = init_l_layer_parameters([2,5,4,3,2,1])\n",
    "for i in parameters:\n",
    "    print(f'{i} has {parameters[i]} and shape {parameters[i].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2a106-1cf8-40a9-a3e2-ddd9756b22a5",
   "metadata": {},
   "source": [
    "<b>Forward propagation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8a575de-354c-491b-ba41-a1177e4b9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    '''\n",
    "    perform linear forward propagation\n",
    "    \n",
    "    Args:\n",
    "    A -- previous layer input\n",
    "    W -- current layer weights\n",
    "    b -- current layer bias\n",
    "    \n",
    "    Return:\n",
    "    Z -- linear forward output \n",
    "    linear_cache -- intermediate variable for backward propagation\n",
    "    '''\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    linear_cache = (A, W, b)\n",
    "    \n",
    "    return Z, linear_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d6917ea-0c31-4d8c-8bed-0f53373d9cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear forward output:\n",
      " [[-0.00875679 -0.00956643 -0.00273312  0.0153382 ]] and shape (1, 4)\n"
     ]
    }
   ],
   "source": [
    "# test linear_forward\n",
    "\n",
    "A_test = np.random.randn(3,4)\n",
    "W_test = np.random.randn(1,3) * 0.01\n",
    "b_test = np.zeros((1,1))\n",
    "Z_test, linear_cache = linear_forward(A_test, W_test, b_test)\n",
    "\n",
    "print(f'linear forward output:\\n {Z_test} and shape {Z_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c2e60fe-da9d-4c50-8231-a60d5b614885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    '''\n",
    "    perform linear forward propagation with non-linear\n",
    "    \n",
    "    Args:\n",
    "    A_prev -- previous layer input\n",
    "    W -- current layer weights\n",
    "    b -- current layer bais\n",
    "    activation -- type of activation function\n",
    "    \n",
    "    Return:\n",
    "    A -- linear_activation output\n",
    "    linear_activation_cache -- intermediate variable for backward propagation\n",
    "    '''\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    linear_activation_cache = (linear_cache, activation_cache)\n",
    "    return A, linear_activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "574ff667-dd50-4b50-88a0-7ca0f8a1ee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid output:\n",
      " [[0.49781082 0.49760841 0.49931672 0.50383447]] and shape (1, 4)\n",
      "relu output:\n",
      " [[0.        0.        0.        0.0153382]] and shape (1, 4)\n"
     ]
    }
   ],
   "source": [
    "# test linear_activation forward function\n",
    "\n",
    "sigmoid_output, sigmoid_linear_activation_cache = linear_activation_forward(A_test, W_test, b_test, 'sigmoid')\n",
    "relu_output, relu_linear_activation_cache = linear_activation_forward(A_test, W_test, b_test, 'relu')\n",
    "print(f'sigmoid output:\\n {sigmoid_output} and shape {sigmoid_output.shape}')\n",
    "print(f'relu output:\\n {relu_output} and shape {relu_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "44355191-d759-4566-bc90-751ef0ef9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-layer linear_activation_forward\n",
    "\n",
    "def l_layer_linear_activation_forward(X, parameters):\n",
    "    '''\n",
    "    perform l-pass forward propagation with non-linear\n",
    "    \n",
    "    Args:\n",
    "    X -- original input\n",
    "    parameters -- a dictionary contains parameters\n",
    "    \n",
    "    Return:\n",
    "    A -- linear_activation output\n",
    "    cache -- intermediate variable for backward propagation\n",
    "    '''\n",
    "    A = X \n",
    "    L = len(parameters) // 2\n",
    "    cache = []\n",
    "    for i in range(1, L):\n",
    "        A_prev = A\n",
    "        A, linear_activation_cache = linear_activation_forward(A_prev, parameters['W'+str(i)], parameters['b'+str(i)], 'relu')\n",
    "        cache.append(linear_activation_cache)\n",
    "        \n",
    "    A_out, linear_activation_cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], 'sigmoid')\n",
    "    cache.append(linear_activation_cache)\n",
    "        \n",
    "    return A_out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c618e9d8-3300-47e5-b519-b89a5c1283df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the network output:\n",
      " [[0.5 0.5 0.5]] and has shape (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# test l-layer linear_activation_forward\n",
    "\n",
    "output, cache = l_layer_linear_activation_forward(np.random.randn(2,3), parameters)\n",
    "print(f'the network output:\\n {output} and has shape {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454582c2-aa0b-4e44-94a9-9393720f7822",
   "metadata": {},
   "source": [
    "<b>Cost function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "631b45c7-99d1-4915-91bf-7d135a346330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y, Y_predict):\n",
    "    '''\n",
    "    compute model cost\n",
    "    \n",
    "    Args:\n",
    "    Y -- ground-truth labels\n",
    "    Y_predict -- model output\n",
    "    \n",
    "    Return:\n",
    "    cost -- model cost\n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(Y * np.log(Y_predict) + (1-Y) * np.log(1 - Y_predict)) / m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85ab3f88-ed7a-4e6c-bf6c-6bee307efea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model cost: 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# test cost\n",
    "\n",
    "cost = compute_cost(np.zeros((1,3)), output)\n",
    "print(f'model cost: {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7f9ad-63b5-4880-8631-2bf2f8a114fc",
   "metadata": {},
   "source": [
    "<b>Back propagation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c8c0adce-8df3-4efa-a7b9-39646198ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_backward\n",
    "\n",
    "def linear_backward(dZ, linear_cache):\n",
    "    '''\n",
    "    perform linear backward propagation\n",
    "    \n",
    "    Args:\n",
    "    dZ -- gradient for Z\n",
    "    linear_cache -- intermediate variables for backward propagation\n",
    "    \n",
    "    Return:\n",
    "    linear_grads -- gradients\n",
    "    '''\n",
    "    \n",
    "    A, W, b = linear_cache\n",
    "    m = A.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ, A.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ) \n",
    "    \n",
    "    linear_grads = {\n",
    "        'dW': dW,\n",
    "        'db': db,\n",
    "        'dA_prev': dA_prev\n",
    "    }\n",
    "    \n",
    "    return linear_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e993255d-4756-45df-b4a7-a3caca3ebdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gradients for weights:\n",
      " [[-0.12309023 -0.16478042 -0.11099738]] and has shape (1, 3)\n",
      "the gradients for bias:\n",
      " [[1.00142954]] and has shape (1, 1)\n",
      "the gradients for previous layer output:\n",
      " [[-0.01110257 -0.01111148 -0.01103627 -0.01083738]\n",
      " [ 0.01154748  0.01155675  0.01147852  0.01127166]\n",
      " [ 0.00909486  0.00910216  0.00904055  0.00887762]] and has shape (3, 4)\n"
     ]
    }
   ],
   "source": [
    "# test linear_backward\n",
    "\n",
    "linear_grads = linear_backward(1-Z_test, linear_cache)\n",
    "print(f'the gradients for weights:\\n {linear_grads[\"dW\"]} and has shape {linear_grads[\"dW\"].shape}')\n",
    "print(f'the gradients for bias:\\n {linear_grads[\"db\"]} and has shape {linear_grads[\"db\"].shape}')\n",
    "print(f'the gradients for previous layer output:\\n {linear_grads[\"dA_prev\"]} and has shape {linear_grads[\"dA_prev\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5985b130-3bb0-40c0-930c-e58d11dd90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, linear_activation_cache, activation):\n",
    "    '''\n",
    "    perform linear backward propagation with non-linear\n",
    "    \n",
    "    Args:\n",
    "    dA -- gradient for A\n",
    "    linear_activation_cache -- intermediate variables for backward propagation\n",
    "    activation -- decide to pick relu or sigmoid\n",
    "    \n",
    "    Return:\n",
    "    linear_activation_grads -- gradients\n",
    "    '''\n",
    "    (linear_cache, activation_cache) = linear_activation_cache\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        linear_activation_grads = linear_backward(dZ, linear_cache)\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        linear_activation_grads = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return linear_activation_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3507190d-a355-4d47-bb76-71a9917bf482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid gradients:\n",
      " {'dW': array([[-0.01477733, -0.02024595, -0.01364713]]), 'db': array([[0.12470584]]), 'dA_prev': array([[-0.00139533, -0.00136982, -0.00137973, -0.00134527],\n",
      "       [ 0.00145124,  0.00142471,  0.00143502,  0.00139918],\n",
      "       [ 0.00114301,  0.00112211,  0.00113023,  0.001102  ]])}\n",
      "relu gradients:\n",
      " {'dW': array([[0., 0., 0.]]), 'db': array([[0.]]), 'dA_prev': array([[-0., -0., -0., -0.],\n",
      "       [ 0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.]])}\n"
     ]
    }
   ],
   "source": [
    "# test linear_activation_backward\n",
    "\n",
    "sigmoid_grads = linear_activation_backward(sigmoid_test, sigmoid_linear_activation_cache, 'sigmoid')\n",
    "relu_grads = linear_activation_backward(relu_test, relu_linear_activation_cache, 'relu')\n",
    "print(f'sigmoid gradients:\\n {sigmoid_grads}')\n",
    "print(f'relu gradients:\\n {relu_grads}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "914bffea-2733-45e6-8438-cc1c70259c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-layer linear_activation_backward\n",
    "\n",
    "def l_layer_linear_activation_backward(Y_predict, Y, cache):\n",
    "    '''\n",
    "    perform l-pass forward propagation with non-linear\n",
    "    \n",
    "    Args:\n",
    "    Y_predict -- forward pass output \n",
    "    Y -- ground-truth label\n",
    "    cache -- intermediate cache\n",
    "    \n",
    "    Return:\n",
    "    grads -- A dictionary with the gradients\n",
    "    '''\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(cache)\n",
    "    Y = Y.reshape(Y_predict.shape)\n",
    "    \n",
    "    dloss = - (np.divide(Y, Y_predict) - np.divide(1-Y, 1-Y_predict))\n",
    "    current_cache = cache[L-1] \n",
    "    linear_activation_grads = linear_activation_backward(dloss, current_cache, 'sigmoid')\n",
    "    dW = linear_activation_grads['dW']\n",
    "    db = linear_activation_grads['db']\n",
    "    dA_prev = linear_activation_grads['dA_prev']\n",
    "    grads['dW' + str(L)] = dW \n",
    "    grads['db' + str(L)] = db \n",
    "    grads['dA' + str(L-1)] = dA_prev \n",
    "    \n",
    "    for i in reversed(range(L-1)):\n",
    "        current_cache = cache[i]\n",
    "        linear_activation_grads = linear_activation_backward(grads['dA' + str(i+1)], current_cache, 'relu')\n",
    "        dW = linear_activation_grads['dW']\n",
    "        db = linear_activation_grads['db']\n",
    "        dA_prev = linear_activation_grads['dA_prev']\n",
    "        grads['dW' + str(i+1)] = dW\n",
    "        grads['db' + str(i+1)] = db\n",
    "        grads['dA' + str(i)] = dA_prev\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "89b970b9-6242-4813-8da6-5bf42cc3e4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads {'dW5': array([[0., 0.]]), 'db5': array([[-0.16666667]]), 'dA4': array([[ 0.01124129, -0.01124129,  0.01124129],\n",
      "       [ 0.00133881, -0.00133881,  0.00133881]]), 'dW4': array([[0., 0., 0.],\n",
      "       [0., 0., 0.]]), 'db4': array([[0.],\n",
      "       [0.]]), 'dA3': array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]]), 'dW3': array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), 'db3': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'dA2': array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]]), 'dW2': array([[0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.]]), 'db2': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'dA1': array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]]), 'dW1': array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]), 'db1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'dA0': array([[0., 0., 0.],\n",
      "       [0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "# test l-layer linear_activation_backward\n",
    "\n",
    "grads = l_layer_linear_activation_backward(output, np.array([[1, 0, 1]]), cache)\n",
    "print(f'grads {grads}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0c66a-5e75-40ab-83e5-f815f6454cce",
   "metadata": {},
   "source": [
    "<b>Update parameters</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "555910e7-a2cc-49f6-ae15-6018a322f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(parameters, grads, learning_rate):\n",
    "    '''\n",
    "    update weights and bias\n",
    "    \n",
    "    Args:\n",
    "    parameters -- weights and bias\n",
    "    grads -- gradients for weights and bias\n",
    "    learning_rate -- learning rate alpha\n",
    "    \n",
    "    Return:\n",
    "    updated_parameters -- updated parameters\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    updated_parameters = parameters\n",
    "    for i in range(L):\n",
    "        updated_parameters['W' + str(i+1)] = parameters['W' + str(i+1)] - learning_rate * grads['dW' + str(i+1)]\n",
    "        updated_parameters['b' + str(i+1)] = parameters['b' + str(i+1)] - learning_rate * grads['db' + str(i+1)]\n",
    "    \n",
    "    return updated_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ba49363e-9ffd-4846-b28c-9a447a0ebc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated parameters:\n",
      " {'W1': array([[ 0.01788628,  0.0043651 ],\n",
      "       [ 0.00096497, -0.01863493],\n",
      "       [-0.00277388, -0.00354759],\n",
      "       [-0.00082741, -0.00627001],\n",
      "       [-0.00043818, -0.00477218]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[-0.01313865,  0.00884622,  0.00881318,  0.01709573,  0.00050034],\n",
      "       [-0.00404677, -0.0054536 , -0.01546477,  0.00982367, -0.01101068],\n",
      "       [-0.01185047, -0.0020565 ,  0.01486148,  0.00236716, -0.01023785],\n",
      "       [-0.00712993,  0.00625245, -0.00160513, -0.00768836, -0.00230031]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[ 0.00745056,  0.01976111, -0.01244123, -0.00626417],\n",
      "       [-0.00803766, -0.02419083, -0.00923792, -0.01023876],\n",
      "       [ 0.01123978, -0.00131914, -0.01623285,  0.00646675]]), 'b3': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W4': array([[-0.00356271, -0.01743141, -0.0059665 ],\n",
      "       [-0.00588594, -0.00873882,  0.00029714]]), 'b4': array([[0.],\n",
      "       [0.]]), 'W5': array([[-0.02248258, -0.00267762]]), 'b5': array([[0.00666667]])}\n"
     ]
    }
   ],
   "source": [
    "# test update parameters\n",
    "\n",
    "updated_parameters = update_params(parameters, grads, 0.01)\n",
    "print(f'updated parameters:\\n {updated_parameters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80339012-7510-4b68-96b3-1bcbb77cba2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
